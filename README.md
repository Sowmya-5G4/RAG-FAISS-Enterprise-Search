ğŸ“˜ Retrieval-Augmented Generation (RAG) System with FAISS, Hybrid Routing & FastAPI
Overview

This project implements a production-style Retrieval-Augmented Generation (RAG) system from scratch, focusing on correct retrieval, explainability, and real-world system design rather than prompt-only demos.

The system supports:

Semantic document retrieval using embeddings and FAISS

Hybrid query routing for structured (CSV) and unstructured (text) data

Cross-encoder reranking for improved retrieval precision

Confidence scoring and source citations

Optional local LLM-based answer generation using Ollama

FastAPI deployment for real API-based usage

The primary goal is to demonstrate how enterprise-grade knowledge search and analytics systems are designed, evaluated, and deployed.

ğŸ§  High-Level Architecture
User Query
   â†“
Query Normalization
   â†“
Query Router
   â”œâ”€â”€ Structured Queries â†’ Pandas (CSV analytics)
   â””â”€â”€ Semantic Queries
         â†“
     FAISS Vector Search
         â†“
     Cross-Encoder Reranking
         â†“
     Context Selection
         â†“
(Optional) Local LLM Generation (Ollama)
         â†“
Answer + Confidence + Sources

ğŸ“ Project Structure
RAG Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ docs/                 # Unstructured knowledge base (text files)
â”‚   â”œâ”€â”€ tables/               # Structured CSV data
â”‚   â””â”€â”€ eval_queries.json     # Evaluation dataset
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py                # FastAPI service
â”‚   â”œâ”€â”€ ingest.py             # Document loading
â”‚   â”œâ”€â”€ chunk.py              # Document chunking
â”‚   â”œâ”€â”€ embed_faiss.py        # Embeddings + FAISS index creation
â”‚   â”œâ”€â”€ retrieve_faiss.py     # FAISS-based retriever
â”‚   â”œâ”€â”€ rerank.py             # Cross-encoder reranking
â”‚   â”œâ”€â”€ router.py             # Query classification
â”‚   â”œâ”€â”€ structured_qa.py      # CSV-based analytics
â”‚   â”œâ”€â”€ generate_ollama.py    # Local LLM generation (Ollama)
â”‚   â”œâ”€â”€ evaluate.py           # Precision@K / Recall@K evaluation
â”‚   â””â”€â”€ rag_pipeline.py       # CLI pipeline (isolated from API)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ” Key Design Decisions
1ï¸âƒ£ Why FAISS?

FAISS (Facebook AI Similarity Search) is used as the vector database because:

Extremely fast similarity search

Scales from small demos to millions of vectors

Fully local and free (no external services required)

Widely used in research and production systems

FAISS cleanly separates vector search from metadata storage, mirroring real-world architectures.

2ï¸âƒ£ Why Hybrid RAG (Text + CSV)?

Real enterprise systems must answer both:

Semantic questions (policies, documentation, knowledge bases)

Analytical questions (metrics, KPIs, incidents)

This project routes queries intelligently to:

FAISS-based semantic retrieval for unstructured text

Pandas-based computation for structured CSV data

3ï¸âƒ£ Why Reranking?

FAISS optimizes for speed, not perfect accuracy.

A cross-encoder reranker is applied after retrieval to:

Improve precision

Reduce irrelevant context

Reflect enterprise RAG best practices

4ï¸âƒ£ Why Ollama for Generation?

LLM generation is intentionally optional and modular.

Ollama is used because:

Fully local (no API keys or quotas)

Supports multiple open models

Easy to swap or disable

If generation fails, the system still returns retrieved context, confidence, and sources.

ğŸ“Š Retrieval Evaluation

The project includes offline evaluation using:

Precision@K

Recall@K

This enables:

Measuring retrieval quality

Validating reranking improvements

Data-driven iteration

ğŸš€ FastAPI Service

The RAG system is deployed as a FastAPI service.

Start the API
uvicorn src.api:app --reload

Swagger UI
http://127.0.0.1:8000/docs

Example Request
{
  "question": "How does security apply to machine learning systems?"
}

Example Response
{
  "answer": "Security applies to machine learning systems by enforcing access control...",
  "confidence": "High (0.82)",
  "sources": ["security_ml.txt", "ml_platform.txt"],
  "route": "semantic"
}

ğŸ§ª Running Locally (CLI)
python -m src.rag_pipeline

ğŸ› ï¸ Installation
pip install -r requirements.txt


If generation is enabled, ensure Ollama is running:

ollama serve

âš ï¸ Notes on Generation

Local LLM generation may time out depending on model size and hardware

Retrieval, confidence scoring, and citations work independently of generation

This behavior is intentional and production-safe

âœ… What This Project Demonstrates

End-to-end RAG system design

FAISS vector database usage

Hybrid structured + unstructured querying

Cross-encoder reranking

Retrieval evaluation metrics

Production-safe error handling

FastAPI deployment

Modular, extensible architecture

ğŸ”® Future Enhancements

Streaming responses

Dockerized deployment

Authentication & rate limiting

UI frontend

Multiple vector backends

ğŸ‘¤ Author

Built as a learning and portfolio project to demonstrate real-world Retrieval-Augmented Generation system design and backend deployment practices.